{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSAMtxMKN5Wr"
   },
   "source": [
    "Este código genera los dataset de DB3, para las particiones de datos por sujeto y por repetición. Se incluye código de visualización para verificar el funcionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pto58N9PElQv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os\n",
    "import random\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de variables y funciones\n",
    "\n",
    "- input_sequence_length: se elige que sea de 650 muestras (3.25s) considerando que cada gesto dura 5s.\n",
    "- stride: se elige 5 ya que se vio que tomar más secuencias tiene un efecto positivo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas\n",
    "ruta_datos = \"../data/\"    # defino ruta a los datos \n",
    "nombre_datos = 'data_gestos_puntuales_fs200_DB3.pickle'\n",
    "\n",
    "# Conformación de datasets\n",
    "input_sequence_length = 650     # largo de las secuencias en muestras\n",
    "stride = 5      # stride de las secuencias\n",
    "sup_gesto_secuencia = 1     # superposición gesto-secuencia de 100%. sólo considero las secuencias en las que hay gesto en todas las muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función para unir datos de entrada, labels y vector de pesos\n",
    "def combine_features_labels_weights(features, sample_weights):\n",
    "    inputs, labels = features\n",
    "    return inputs, labels, sample_weights\n",
    "\n",
    "# función para identificar a qué repetición corresponde una secuencia\n",
    "def find_most_similar_array_index(target_array, list_of_arrays):\n",
    "  max_common_elements = 0\n",
    "  most_similar_index = -1\n",
    "\n",
    "  for index, array in enumerate(list_of_arrays):\n",
    "      # Convert both arrays to sets\n",
    "      target_set = set(target_array)\n",
    "      array_set = set(array)\n",
    "\n",
    "      # Find the number of common elements\n",
    "      common_elements = len(target_set.intersection(array_set))\n",
    "\n",
    "      # Update the most similar array index if this one has more common elements\n",
    "      if common_elements > max_common_elements:\n",
    "          max_common_elements = common_elements\n",
    "          most_similar_index = index\n",
    "\n",
    "  return most_similar_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIA58wx1UsD-"
   },
   "source": [
    "# Levanto los datos del Experimento 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8453,
     "status": "ok",
     "timestamp": 1738882003669,
     "user": {
      "displayName": "Damián Charizard",
      "userId": "06800944873226630061"
     },
     "user_tz": 180
    },
    "id": "lWcC4cHhEln2",
    "outputId": "8cd24f30-5d8b-41bf-f585-1d26f4db38a3"
   },
   "outputs": [],
   "source": [
    "# Cargo los datos\n",
    "os.chdir(ruta_datos) # me situo en el directorio\n",
    "file = open(nombre_datos, 'rb') # abro el archivo\n",
    "MyoArm_data = pickle.load(file) # guardo los datos en un data y cierro el archivo\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNFLOZWSSBCT"
   },
   "source": [
    "## Extracción de datos y corrección de etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "00kE9d80RsLV"
   },
   "outputs": [],
   "source": [
    "# Extraigo los datos del dataframe\n",
    "data = MyoArm_data['data']\n",
    "label = MyoArm_data['label']\n",
    "subject = MyoArm_data['subject']\n",
    "\n",
    "nOfSubjects = np.max(subject)   # calculo la cantidad de sujetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1738882003730,
     "user": {
      "displayName": "Damián Charizard",
      "userId": "06800944873226630061"
     },
     "user_tz": 180
    },
    "id": "7vx_rXT-Y0ee",
    "outputId": "16841203-4b20-4c1a-8dce-b07dc9971cb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  5  6  7  9 10 13 14 22 26 31]\n",
      "[1903645   69264   69408   69342   69415   68690   69347   69613   68545\n",
      "   68807   63080]\n"
     ]
    }
   ],
   "source": [
    "# Observo cuantos datos tengo de cada etiqueta, viendo una gran cantidad de reposos, y una cantidad similar entre el resto de clases\n",
    "values, counts = np.unique(label, return_counts=True)\n",
    "print(values)   # imprimo valores de etiquetas\n",
    "print(counts)   # imprimo la cantidad de datos de cada etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "T_Yh0LQgbSL1"
   },
   "outputs": [],
   "source": [
    "# TODO no hardcodear el mapeo\n",
    "# Creo un diccionario de mapeo para los valores de etiqueta\n",
    "mapeo = {0: 0, 5: 1, 6: 2, 7: 3, 9: 4, 10: 5, 13: 6, 14: 7, 22: 8, 26: 9, 31: 10}\n",
    "\n",
    "# Hago el reemplazo de las etiquetas\n",
    "label = np.vectorize(mapeo.get)(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas no mapeadas: {np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(8)}\n"
     ]
    }
   ],
   "source": [
    "# Si hay clases que no están, identificarlas\n",
    "etiquetas_no_mapeadas = set(label) - set(mapeo.keys())\n",
    "print(\"Etiquetas no mapeadas:\", etiquetas_no_mapeadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "988JqZqAz7k0"
   },
   "source": [
    "# Rellenado de datos\n",
    "\n",
    "Relleno datos faltantes según una distribución normal de media y desviación estándar igual a la de los datos del reposo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Vp-zSAQnZE8m"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aplicar_imputer(data, label):\n",
    "  # calculo desviacion estandar sobre los datos de reposo\n",
    "  ind_reposo = np.where(label==0)[0]\n",
    "\n",
    "  std_reposo = np.nanstd(data[ind_reposo], axis=0)\n",
    "  mean_reposo = np.nanmean(data[ind_reposo], axis=0)\n",
    "\n",
    "  # relleno manualmente los NaN en cada columna con los valores de relleno\n",
    "  for i in range(data.shape[1]):\n",
    "      nan_mask = np.isnan(data[:, i])  # máscara de los NaN en la columna i\n",
    "      data[nan_mask, i] = std_reposo[i]*np.random.normal() + mean_reposo[i]  # reemplazo los NaN por el valor de relleno correspondiente\n",
    "\n",
    "  return data\n",
    "\n",
    "data = aplicar_imputer(data, label)\n",
    "\n",
    "# Para termianr, chequeo si hay alguna etiqueta nan\n",
    "nan_indices = np.isnan(data)\n",
    "np.where(nan_indices==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hago una lista de dataframes, donde cada elemento tiene los datos asociados a los distintos sujetos \n",
    "list_data_x_sujeto = []\n",
    "list_label_x_sujeto = []\n",
    "\n",
    "for i in range(1, nOfSubjects+1):\n",
    "  ind_sujeto_act = np.where(subject==i)[0]\n",
    "  list_data_x_sujeto.append(data[ind_sujeto_act])\n",
    "  list_label_x_sujeto.append(label[ind_sujeto_act])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo del vector de pesos\n",
    "Hago una simulación de armar los datasets para ver cuántas secuencias queda de cada clase. No se tiene en cuenta el reposo, por que lo ahora la numeración de las clases se mueve un lugar a la izquierda: la que antes era la clase 1, ahora es la 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos de clase: {0: np.float64(0.9884702764104506), 1: np.float64(0.9814097744360902), 2: np.float64(0.9839992461364493), 3: np.float64(0.9830728676332141), 4: np.float64(1.0096886482305163), 5: np.float64(0.9839992461364493), 6: np.float64(0.9757241637077182), 7: np.float64(1.01498833592535), 8: np.float64(1.0048306389530408), 9: np.float64(1.0823175787728025)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pesos_x_clase.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO no hacer una simulación de armar los datasets, sino que hacerlo más derecho fijándome cuántas secuencias cumplen la condición de superposición gesto-secuencia\n",
    "# para cada sujeto armo un Dataset con las secuencias de datos y etiquetas\n",
    "list_labels_dataset_x_sujeto = []\n",
    "\n",
    "for j in range(nOfSubjects):#\n",
    "  label_j = list_label_x_sujeto[j]\n",
    "\n",
    "  label_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
    "          data=label_j,  # excluimos los últimos target_sequence_length elementos de los datos\n",
    "          targets=None,\n",
    "          sequence_length=input_sequence_length,\n",
    "          batch_size=None,   # multiplo de 8\n",
    "          sequence_stride = stride\n",
    "        )\n",
    "\n",
    "  secuencias_labels_list = []\n",
    "\n",
    "  for x in label_dataset:\n",
    "    secuencias_labels_list.append(x)\n",
    "\n",
    "  selected_secuencias_labels_list = []\n",
    "\n",
    "  contador_reposos = 0\n",
    "\n",
    "  for i in range(len(secuencias_labels_list)):  # len(secuencias_labels_list)\n",
    "    unique_values, counts = np.unique(secuencias_labels_list[i], return_counts=True)\n",
    "    # Encontrar el valor que más se repite\n",
    "    max_count_index = np.argmax(counts)\n",
    "    most_frequent_value = unique_values[max_count_index]\n",
    "    counts_most_frequent = counts[max_count_index]\n",
    "\n",
    "    if most_frequent_value==0:\n",
    "      contador_reposos += 1\n",
    "\n",
    "    if (((counts_most_frequent/len(secuencias_labels_list[i])) == sup_gesto_secuencia) and (most_frequent_value != 0)):\n",
    "      selected_secuencias_labels_list.append(most_frequent_value)\n",
    "\n",
    "  list_labels_dataset_x_sujeto.extend(selected_secuencias_labels_list)\n",
    "  \n",
    "  unique_values, counts = np.unique(list_labels_dataset_x_sujeto, return_counts=True)\n",
    "# Calcula los pesos de clase\n",
    "class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                    classes=np.unique(list_labels_dataset_x_sujeto), y=np.array(list_labels_dataset_x_sujeto))\n",
    "# Convierte los pesos a un diccionario\n",
    "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "print(\"Pesos de clase:\", class_weights_dict) \n",
    "\n",
    "joblib.dump(class_weights_dict, \"pesos_x_clase.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partición de datos por sujeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de datasets sin normalización\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accedo al vector de pesos\n",
    "class_weights_dict = joblib.load(\"pesos_x_clase.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(nOfSubjects):#\n",
    "  label_j = list_label_x_sujeto[j]\n",
    "  data_j = list_data_x_sujeto[j]\n",
    "\n",
    "  label_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
    "          data=label_j,  # excluimos los últimos target_sequence_length elementos de los datos\n",
    "          targets=None,\n",
    "          sequence_length=input_sequence_length,\n",
    "          batch_size=None,   # potencia de 2\n",
    "          sequence_stride = stride\n",
    "        )\n",
    "  data_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
    "          data=data_j,  # excluimos los últimos target_sequence_length elementos de los datos\n",
    "          targets=None,\n",
    "          sequence_length=input_sequence_length,\n",
    "          batch_size=None,   # potencia de 2\n",
    "          sequence_stride = stride\n",
    "        )\n",
    "\n",
    "  secuencias_labels_list = []\n",
    "  secuencias_data_list = []\n",
    "\n",
    "  for x in label_dataset:\n",
    "    secuencias_labels_list.append(x)\n",
    "\n",
    "  for x in data_dataset:\n",
    "    secuencias_data_list.append(x)\n",
    "\n",
    "  selected_secuencias_labels_list = []\n",
    "  selected_secuencias_data_list = []\n",
    "\n",
    "  contador_reposos = 0\n",
    "\n",
    "  for i in range(len(secuencias_labels_list)):\n",
    "    unique_values, counts = np.unique(secuencias_labels_list[i], return_counts=True)\n",
    "    # encuentro el valor que más se repite\n",
    "    max_count_index = np.argmax(counts)\n",
    "    most_frequent_value = unique_values[max_count_index]\n",
    "    counts_most_frequent = counts[max_count_index]\n",
    "\n",
    "    if most_frequent_value==0:\n",
    "      contador_reposos += 1\n",
    "\n",
    "    if (((counts_most_frequent/len(secuencias_labels_list[i])) == sup_gesto_secuencia) and (most_frequent_value != 0)): # la segunda condicion es para excluir el reposo\n",
    "      selected_secuencias_labels_list.append(most_frequent_value)\n",
    "      selected_secuencias_data_list.append(secuencias_data_list[i])\n",
    "\n",
    "  selected_secuencias_labels_array = np.array(selected_secuencias_labels_list)\n",
    "\n",
    "  # creo una matriz de pesos de la misma forma que y_train\n",
    "  sample_weight = np.zeros(selected_secuencias_labels_array.shape)\n",
    "\n",
    "  selected_secuencias_labels_array = selected_secuencias_labels_array - 1\n",
    "\n",
    "  # asigno los pesos de clase a cada timestep en la secuencia\n",
    "  for i, class_weight in class_weights_dict.items():\n",
    "      sample_weight[selected_secuencias_labels_array == i] = class_weight\n",
    "\n",
    "  # convierto las listas en tensores de TensorFlow\n",
    "  tensores = tf.stack(selected_secuencias_data_list)\n",
    "  etiquetas = tf.convert_to_tensor(list(selected_secuencias_labels_array))\n",
    "\n",
    "  sample_weight_tensor = tf.convert_to_tensor(sample_weight, dtype=tf.float32)\n",
    "  sample_weight_dataset = tf.data.Dataset.from_tensor_slices(sample_weight_tensor)\n",
    "\n",
    "  # creo un dataset a partir de los tensores y etiquetas\n",
    "  input_dataset = tf.data.Dataset.from_tensor_slices((tensores, etiquetas))\n",
    "  # agrego el vector de pesos\n",
    "  rnn_dataset_j = tf.data.Dataset.zip((input_dataset, sample_weight_dataset))\n",
    "  rnn_dataset_j_def = rnn_dataset_j.map(combine_features_labels_weights)\n",
    "\n",
    "  # guardo el dataset\n",
    "  rnn_dataset_j_def.save(\"dataset_x_sujeto_\"+str(input_sequence_length) + \"_stride\" + str(stride) + \"_sub\"+str(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partición de datos por repetición"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo y visualización de las repeticiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculo cantidad de repeticiones de cada gesto\n",
    "flancos = np.diff(label)  # con estos flancos puedo dividir las repeticiones\n",
    "rep_starts_ind = np.where(flancos>0)[0]\n",
    "rep_ends_ind = np.where(flancos<0)[0]\n",
    "\n",
    "# identifico y numero cada repetición\n",
    "etiqueta_repeticion = []  # identificacion de cada repeticion\n",
    "indices_repeticion = []   # que muestras conforman cada repeticion\n",
    "\n",
    "for i in range(1, 11):\n",
    "  rep_start_ind = np.where(flancos==i)[0]\n",
    "  rep_end_ind = np.where(flancos==-i)[0]\n",
    "  for j in range(len(rep_start_ind)):\n",
    "    indices_repeticion.append(np.arange(rep_start_ind[j]+1, rep_end_ind[j]+1))\n",
    "    etiqueta_repeticion.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.array(label)\n",
    "\n",
    "# cantidad de muestras extra antes y después de cada repetición\n",
    "contexto = 1500  # se puede ajustar este valor\n",
    "num_to_plot = 5  # elijo cuántas repeticiones quiero mostrar\n",
    "ticks_y = np.arange(0, np.max(label)+2, 1)  # escala más detallada en Y\n",
    "\n",
    "# Elegimos repeticiones al azar\n",
    "num_reps = len(indices_repeticion)\n",
    "reps_to_plot = random.sample(range(num_reps), num_to_plot)\n",
    "\n",
    "plt.figure(figsize=(15, num_to_plot * 2.5))\n",
    "\n",
    "for i, rep_idx in enumerate(reps_to_plot):\n",
    "    idxs = indices_repeticion[rep_idx]\n",
    "    etiqueta = etiqueta_repeticion[rep_idx]\n",
    "\n",
    "    start = max(idxs[0] - contexto, 0)\n",
    "    end = min(idxs[-1] + contexto, len(label))\n",
    "\n",
    "    rango = np.arange(start, end)\n",
    "\n",
    "    plt.subplot(num_to_plot, 1, i+1)\n",
    "    plt.plot(rango, label[rango], label=f\"label\", color='tab:blue', linewidth=2)\n",
    "    plt.axvspan(idxs[0], idxs[-1], color='orange', alpha=0.3, label='Repetición')\n",
    "    plt.title(f\"Repetición #{rep_idx} - Gesto {etiqueta}\")\n",
    "    plt.xlabel(\"Índice de muestra\")\n",
    "    plt.ylabel(\"Etiqueta\")\n",
    "    plt.yticks(ticks_y)\n",
    "    plt.ylim([min(ticks_y), max(ticks_y)])\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de datasets\n",
    "Se necesita que las repeticiones de un mismo gesto se encuentren juntas y sean 6 o menos (se tienen 6 folds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: np.float64(0.9884702764104506), 1: np.float64(0.9814097744360902), 2: np.float64(0.9839992461364493), 3: np.float64(0.9830728676332141), 4: np.float64(1.0096886482305163), 5: np.float64(0.9839992461364493), 6: np.float64(0.9757241637077182), 7: np.float64(1.01498833592535), 8: np.float64(1.0048306389530408), 9: np.float64(1.0823175787728025)}\n"
     ]
    }
   ],
   "source": [
    "class_weights_dict = joblib.load(\"pesos_x_clase.pkl\")\n",
    "print(class_weights_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de funcionamiento\n",
    "Hago todo menos generar los datasets para guardarlos. Permite ver que la partición por repetición se está realizando correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizacion = False    # defino si quiero normalizar por secuencia\n",
    "j = 1   # selecciono el sujeto\n",
    "\n",
    "data_j = list_data_x_sujeto[j]\n",
    "label_j = list_label_x_sujeto[j]\n",
    "\n",
    "flancos = np.diff(label_j)  # Con estos flancos puedo dividir las repeticiones\n",
    "rep_starts_ind = np.where(flancos>0)[0]\n",
    "rep_ends_ind = np.where(flancos<0)[0]\n",
    "\n",
    "etiqueta_repeticion = []\n",
    "indices_repeticion = []\n",
    "\n",
    "etiqueta_repeticion = label_j[rep_ends_ind]\n",
    "\n",
    "for k in range(len(rep_starts_ind)):\n",
    "  # deberia quedarme del mismo largo que etiqueta_repeticion\n",
    "  indices_k = np.arange(rep_starts_ind[k]+1, rep_ends_ind[k]+1) # creo un vector de indices tomando el inicio y fin\n",
    "  indices_repeticion.append(indices_k)\n",
    "  \n",
    "indices_label = np.arange(0, len(label_j))\n",
    "\n",
    "# armo secuencias con el vector de etiquetas label\n",
    "label_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        data=label_j,  # excluimos los últimos target_sequence_length elementos de los datos\n",
    "        targets=None,\n",
    "        sequence_length=input_sequence_length,\n",
    "        batch_size=None,   # potencia de 2\n",
    "        sequence_stride = stride\n",
    "      )\n",
    "\n",
    "# armo secuencias de los datos de EMG \n",
    "data_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        data=data_j,  # excluimos los últimos target_sequence_length elementos de los datos\n",
    "        targets=None,\n",
    "        sequence_length=input_sequence_length,\n",
    "        batch_size=None,   # potencia de 2\n",
    "        sequence_stride = stride\n",
    "      )\n",
    "\n",
    "# armo secuencias de los indices de muestras de cada secuencia\n",
    "indices_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        data=indices_label,  # excluimos los últimos target_sequence_length elementos de los datos\n",
    "        targets=None,\n",
    "        sequence_length=input_sequence_length,\n",
    "        batch_size=None,   # potencia de 2\n",
    "        sequence_stride = stride\n",
    "      )\n",
    "\n",
    "secuencias_labels_list = []\n",
    "secuencias_data_list = []\n",
    "indices_dataset_list = []\n",
    "\n",
    "for x in label_dataset:\n",
    "  secuencias_labels_list.append(x)\n",
    "\n",
    "for x in data_dataset:\n",
    "  secuencias_data_list.append(x)\n",
    "\n",
    "for x in indices_dataset:\n",
    "  indices_dataset_list.append(x)\n",
    "\n",
    "selected_secuencias_labels_list = []\n",
    "selected_secuencias_data_list = []\n",
    "selected_secuencias_indices_list = []\n",
    "valores = []\n",
    "\n",
    "contador_reposos = 0\n",
    "\n",
    "for i in range(len(secuencias_labels_list)):  # len(secuencias_labels_list)\n",
    "  unique_values, counts = np.unique(secuencias_labels_list[i], return_counts=True)\n",
    "  # Encontrar el valor que más se repite\n",
    "  max_count_index = np.argmax(counts)\n",
    "  most_frequent_value = unique_values[max_count_index]\n",
    "  counts_most_frequent = counts[max_count_index]\n",
    "  valores.append(most_frequent_value)\n",
    "  if most_frequent_value==0:\n",
    "    contador_reposos += 1\n",
    "\n",
    "  if (((counts_most_frequent/len(secuencias_labels_list[i])) == sup_gesto_secuencia) and (most_frequent_value != 0)):\n",
    "    selected_secuencias_labels_list.append(most_frequent_value)   # etiqueta de la secuencia\n",
    "    if normalizacion:\n",
    "      selected_secuencias_data_list.append((secuencias_data_list[i]-np.mean(secuencias_data_list[i]))/np.std(secuencias_data_list[i]))  # normalización por secuencia\n",
    "    else:\n",
    "      selected_secuencias_data_list.append(secuencias_data_list[i])\n",
    "    selected_secuencias_indices_list.append(indices_dataset_list[i])\n",
    "\n",
    "# para que inicie en 0 ya que no consideramos el reposo\n",
    "selected_secuencias_labels_array = np.array(selected_secuencias_labels_list)\n",
    "selected_secuencias_labels_array = selected_secuencias_labels_array - 1\n",
    "\n",
    "# Armo vector de pesos\n",
    "# creo una matriz de pesos de la misma forma que y_train\n",
    "sample_weight = np.zeros(selected_secuencias_labels_array.shape)\n",
    "# Asignar los pesos de clase a cada timestep en la secuencia\n",
    "for i, class_weight in class_weights_dict.items():\n",
    "    sample_weight[selected_secuencias_labels_array == i] = class_weight\n",
    "\n",
    "# asignacion de repeticiones a secuencias\n",
    "correspondencia_repe = [] # para cada secuencia guardo la repetición a la que pertenece\n",
    "\n",
    "for i in range(len(selected_secuencias_indices_list)):\n",
    "    # convierto tensores a listas\n",
    "    if isinstance(selected_secuencias_indices_list[i], tf.Tensor):\n",
    "        target_array = selected_secuencias_indices_list[i].numpy().tolist()\n",
    "    else:\n",
    "        target_array = selected_secuencias_indices_list[i].tolist()\n",
    "\n",
    "    list_of_arrays = []\n",
    "    for array in indices_repeticion:\n",
    "        if isinstance(array, tf.Tensor):\n",
    "            list_of_arrays.append(array.numpy().tolist())\n",
    "        else:\n",
    "            list_of_arrays.append(array.tolist())\n",
    "\n",
    "    most_similar_index = find_most_similar_array_index(target_array, list_of_arrays)\n",
    "    correspondencia_repe.append(most_similar_index)\n",
    "    \n",
    "repeticiones_encontradas = np.unique(correspondencia_repe)\n",
    "\n",
    "secuencias_repeticiones_encontradas = []  # en cada elemento quedan los indices de las secuencias de cada repe\n",
    "# me puedo fijar la etiqueta mirando la variable etiqueta_repeticion\n",
    "\n",
    "for i in repeticiones_encontradas:\n",
    "  ind_secuencias_repeticion_i = np.where(correspondencia_repe==i)[0]\n",
    "  secuencias_repeticiones_encontradas.append(ind_secuencias_repeticion_i)\n",
    "\n",
    "if len(secuencias_repeticiones_encontradas) != len(etiqueta_repeticion):\n",
    "  print('Revisar secuencias_repeticiones_encontradas o etiqueta_repeticion')\n",
    "\n",
    "\n",
    "# Armo 6 folds porque tengo 6 repeticiones de cada gesto\n",
    "# voy a usar la logica de repartir las repeticiones por cada fold\n",
    "folds_list = [[], [], [], [], [], []]\n",
    "\n",
    "# voy a ir repartiendo las repeticiones gesto a gesto\n",
    "gesto_actual = 1  # si los gestos están en orden, empieza en 1\n",
    "ind_receptor = 0  # el indice del fold que recibe una repetición\n",
    "\n",
    "# necesito una condicion que chequee que no tenga mas repeticiones que folds\n",
    "# por ahora voy a suponer que tengo maximo 6 repeticiones en cada gesto\n",
    "\n",
    "for i in range(len(secuencias_repeticiones_encontradas)): # (cantidad de repeticiones) iteraciones\n",
    "  # print(i)\n",
    "  if gesto_actual != etiqueta_repeticion[i]:  # si cambio de gesto, hago que el fold 0 sea el receptor\n",
    "    gesto_actual = etiqueta_repeticion[i]\n",
    "    # cambio_gesto = True\n",
    "    ind_receptor = 0\n",
    "    #print(ind_receptor)\n",
    "    folds_list[ind_receptor].append(secuencias_repeticiones_encontradas[i])\n",
    "    ind_receptor += 1\n",
    "  else:\n",
    "    # cambio_gesto = False\n",
    "\n",
    "    # print(ind_receptor)\n",
    "    folds_list[ind_receptor].append(secuencias_repeticiones_encontradas[i])\n",
    "    ind_receptor += 1\n",
    "\n",
    "# lo que sigue tengo que pasarlo a funciones\n",
    "fold_list0 = folds_list[0]\n",
    "fold_list1 = folds_list[1]\n",
    "fold_list2 = folds_list[2]\n",
    "fold_list3 = folds_list[3]\n",
    "fold_list4 = folds_list[4]\n",
    "fold_list5 = folds_list[5]\n",
    "\n",
    "flattened_fold_list0 = np.concatenate(fold_list0)\n",
    "flattened_fold_list1 = np.concatenate(fold_list1)\n",
    "flattened_fold_list2 = np.concatenate(fold_list2)\n",
    "flattened_fold_list3 = np.concatenate(fold_list3)\n",
    "flattened_fold_list4 = np.concatenate(fold_list4)\n",
    "flattened_fold_list5 = np.concatenate(fold_list5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En cada elemento tengo los indices de las secuencias seleccionadas de cada repeticion\n",
    "print(secuencias_repeticiones_encontradas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de repeticiones: 60\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de repeticiones: \" + str(len(secuencias_repeticiones_encontradas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aquí se guardan los índices de las secuencias que va a contener cada fold\n",
    "print(flattened_fold_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# índice de la repetición a graficar\n",
    "repeticion_idx = 20  #  elijo el numero de repetición\n",
    "\n",
    "# contexto (sobrante) antes y después del segmento\n",
    "contexto = 300  # podés ajustar este valor\n",
    "\n",
    "# obtengo índices de las secuencias que forman la repetición\n",
    "indices_de_secuencias = secuencias_repeticiones_encontradas[repeticion_idx]\n",
    "if isinstance(indices_de_secuencias, tf.Tensor):\n",
    "    indices_de_secuencias = indices_de_secuencias.numpy()\n",
    "\n",
    "# reuno todos los índices de muestra de esas secuencias\n",
    "todos_indices = []\n",
    "for idx in indices_de_secuencias:\n",
    "    indices_muestra = selected_secuencias_indices_list[idx]\n",
    "    if isinstance(indices_muestra, tf.Tensor):\n",
    "        indices_muestra = indices_muestra.numpy()\n",
    "    todos_indices.extend(indices_muestra)\n",
    "\n",
    "# determino el rango total a mostrar\n",
    "inicio_total = max(min(todos_indices) - contexto, 0)\n",
    "fin_total = min(max(todos_indices) + contexto, len(label_j))\n",
    "\n",
    "# segmento de label_j a graficar\n",
    "segmento = label_j[inicio_total:fin_total]\n",
    "rango_segmento = np.arange(inicio_total, fin_total)\n",
    "\n",
    "# creo los colores para cada secuencia\n",
    "num_secuencias = len(indices_de_secuencias)\n",
    "colormap = cm.get_cmap('tab10', num_secuencias)\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.plot(rango_segmento, segmento, label=\"label_j\", color='gray', linewidth=1.5)\n",
    "plt.title(f\"Repetición #{repeticion_idx} en label_j (recortado)\")\n",
    "plt.xlabel(\"Índice de muestra\")\n",
    "plt.ylabel(\"Etiqueta\")\n",
    "plt.grid(True)\n",
    "\n",
    "# recorro las secuencias para graficarlas\n",
    "for i, idx in enumerate(indices_de_secuencias):\n",
    "    indices_muestra = selected_secuencias_indices_list[idx]\n",
    "    if isinstance(indices_muestra, tf.Tensor):\n",
    "        indices_muestra = indices_muestra.numpy()\n",
    "    inicio = indices_muestra[0]\n",
    "    fin = indices_muestra[-1]\n",
    "    color = colormap(i)\n",
    "\n",
    "    # fondo de color suave\n",
    "    plt.axvspan(inicio, fin, color=color, alpha=0.3, label=f\"Secuencia {i+1}\")\n",
    "\n",
    "    # bordes punteados\n",
    "    plt.axvline(inicio, color=color, linestyle='--', linewidth=1.2)\n",
    "    plt.axvline(fin, color=color, linestyle='--', linewidth=1.2)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es preciso notar que los resultados son congruentes. Se comienzan a tomar las secuencias una vez que inicia el gesto, y se dejan de considerar cuando la secuencia deja de abarcar el gesto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardar_ds = True     # defino si quiero guardar los datasets\n",
    "normalizacion = False    # defino si quiero normalizar por secuencia\n",
    "nombre_exp = \"dataset_x_rep_DB3_\"+str(input_sequence_length) + \"_stride\" + str(stride) +\"_sub\"    # defino los nombres de los datasets\n",
    "\n",
    "if normalizacion:\n",
    "  nombre_exp += \"_norm_\" \n",
    "\n",
    "for j in range(nOfSubjects - 3, nOfSubjects):  \n",
    "  data_j = list_data_x_sujeto[j]\n",
    "  label_j = list_label_x_sujeto[j]\n",
    "\n",
    "  flancos = np.diff(label_j)  # Con estos flancos puedo dividir las repeticiones\n",
    "  rep_starts_ind = np.where(flancos>0)[0]\n",
    "  rep_ends_ind = np.where(flancos<0)[0]\n",
    "\n",
    "  etiqueta_repeticion = []\n",
    "  indices_repeticion = []\n",
    "\n",
    "  etiqueta_repeticion = label_j[rep_ends_ind]\n",
    "\n",
    "  for k in range(len(rep_starts_ind)):\n",
    "    # deberia quedarme del mismo largo que etiqueta_repeticion\n",
    "    indices_k = np.arange(rep_starts_ind[k]+1, rep_ends_ind[k]+1) # creo un vector de indices tomando el inicio y fin\n",
    "    indices_repeticion.append(indices_k)\n",
    "    \n",
    "  indices_label = np.arange(0, len(label_j))\n",
    "\n",
    "  # armo secuencias con el vector de etiquetas label\n",
    "  label_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
    "          data=label_j,  # excluimos los últimos target_sequence_length elementos de los datos\n",
    "          targets=None,\n",
    "          sequence_length=input_sequence_length,\n",
    "          batch_size=None,   # potencia de 2\n",
    "          sequence_stride = stride\n",
    "        )\n",
    "  \n",
    "  # armo secuencias de los datos de EMG \n",
    "  data_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
    "          data=data_j,  # excluimos los últimos target_sequence_length elementos de los datos\n",
    "          targets=None,\n",
    "          sequence_length=input_sequence_length,\n",
    "          batch_size=None,   # potencia de 2\n",
    "          sequence_stride = stride\n",
    "        )\n",
    "  \n",
    "  # armo secuencias de los indices de muestras de cada secuencia\n",
    "  indices_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
    "          data=indices_label,  # excluimos los últimos target_sequence_length elementos de los datos\n",
    "          targets=None,\n",
    "          sequence_length=input_sequence_length,\n",
    "          batch_size=None,   # potencia de 2\n",
    "          sequence_stride = stride\n",
    "        )\n",
    "\n",
    "  secuencias_labels_list = []\n",
    "  secuencias_data_list = []\n",
    "  indices_dataset_list = []\n",
    "\n",
    "  for x in label_dataset:\n",
    "    secuencias_labels_list.append(x)\n",
    "\n",
    "  for x in data_dataset:\n",
    "    secuencias_data_list.append(x)\n",
    "\n",
    "  for x in indices_dataset:\n",
    "    indices_dataset_list.append(x)\n",
    "\n",
    "  selected_secuencias_labels_list = []\n",
    "  selected_secuencias_data_list = []\n",
    "  selected_secuencias_indices_list = []\n",
    "  valores = []\n",
    "\n",
    "  contador_reposos = 0\n",
    "\n",
    "  for i in range(len(secuencias_labels_list)):  # len(secuencias_labels_list)\n",
    "    unique_values, counts = np.unique(secuencias_labels_list[i], return_counts=True)\n",
    "    # Encontrar el valor que más se repite\n",
    "    max_count_index = np.argmax(counts)\n",
    "    most_frequent_value = unique_values[max_count_index]\n",
    "    counts_most_frequent = counts[max_count_index]\n",
    "    valores.append(most_frequent_value)\n",
    "    if most_frequent_value==0:\n",
    "      contador_reposos += 1\n",
    "\n",
    "    if (((counts_most_frequent/len(secuencias_labels_list[i])) == sup_gesto_secuencia) and (most_frequent_value != 0)):\n",
    "      selected_secuencias_labels_list.append(most_frequent_value)   # etiqueta de la secuencia\n",
    "      if normalizacion:\n",
    "        selected_secuencias_data_list.append((secuencias_data_list[i]-np.mean(secuencias_data_list[i]))/np.std(secuencias_data_list[i]))  # normalización por secuencia\n",
    "      else:\n",
    "        selected_secuencias_data_list.append(secuencias_data_list[i])\n",
    "      selected_secuencias_indices_list.append(indices_dataset_list[i])\n",
    "\n",
    "  # para que inicie en 0 ya que no consideramos el reposo\n",
    "  selected_secuencias_labels_array = np.array(selected_secuencias_labels_list)\n",
    "  selected_secuencias_labels_array = selected_secuencias_labels_array - 1\n",
    "  \n",
    "  # Armo vector de pesos\n",
    "  # creo una matriz de pesos de la misma forma que y_train\n",
    "  sample_weight = np.zeros(selected_secuencias_labels_array.shape)\n",
    "  # Asignar los pesos de clase a cada timestep en la secuencia\n",
    "  for i, class_weight in class_weights_dict.items():\n",
    "      sample_weight[selected_secuencias_labels_array == i] = class_weight\n",
    "\n",
    "  # asignacion de repeticiones a secuencias\n",
    "  correspondencia_repe = [] # para cada secuencia guardo la repetición a la que pertenece\n",
    "\n",
    "  for i in range(len(selected_secuencias_indices_list)):\n",
    "      # convierto tensores a listas\n",
    "      if isinstance(selected_secuencias_indices_list[i], tf.Tensor):\n",
    "          target_array = selected_secuencias_indices_list[i].numpy().tolist()\n",
    "      else:\n",
    "          target_array = selected_secuencias_indices_list[i].tolist()\n",
    "\n",
    "      list_of_arrays = []\n",
    "      for array in indices_repeticion:\n",
    "          if isinstance(array, tf.Tensor):\n",
    "              list_of_arrays.append(array.numpy().tolist())\n",
    "          else:\n",
    "              list_of_arrays.append(array.tolist())\n",
    "\n",
    "      most_similar_index = find_most_similar_array_index(target_array, list_of_arrays)\n",
    "      correspondencia_repe.append(most_similar_index)\n",
    "        \n",
    "  repeticiones_encontradas = np.unique(correspondencia_repe)\n",
    "\n",
    "  secuencias_repeticiones_encontradas = []  # en cada elemento quedan los indices de las secuencias de cada repe\n",
    "  # me puedo fijar la etiqueta mirando la variable etiqueta_repeticion\n",
    "\n",
    "  for i in repeticiones_encontradas:\n",
    "    ind_secuencias_repeticion_i = np.where(correspondencia_repe==i)[0]\n",
    "    secuencias_repeticiones_encontradas.append(ind_secuencias_repeticion_i)\n",
    "\n",
    "  if len(secuencias_repeticiones_encontradas) != len(etiqueta_repeticion):\n",
    "    print('Revisar secuencias_repeticiones_encontradas o etiqueta_repeticion')\n",
    "\n",
    "\n",
    "  # Armo 6 folds porque tengo 6 repeticiones de cada gesto\n",
    "  # voy a usar la logica de repartir las repeticiones por cada fold\n",
    "  folds_list = [[], [], [], [], [], []]\n",
    "\n",
    "  # voy a ir repartiendo las repeticiones gesto a gesto\n",
    "  gesto_actual = 1\n",
    "  ind_receptor = 0  # el indice del fold que recibe una repetición\n",
    "\n",
    "  # necesito una condicion que chequee que no tenga mas repeticiones que folds\n",
    "  # por ahora voy a suponer que tengo maximo 6 repeticiones en cada gesto\n",
    "\n",
    "  for i in range(len(secuencias_repeticiones_encontradas)):\n",
    "    # print(i)\n",
    "    if gesto_actual != etiqueta_repeticion[i]:\n",
    "      gesto_actual = etiqueta_repeticion[i]\n",
    "      # cambio_gesto = True\n",
    "      ind_receptor = 0\n",
    "      #print(ind_receptor)\n",
    "      folds_list[ind_receptor].append(secuencias_repeticiones_encontradas[i])\n",
    "      ind_receptor += 1\n",
    "    else:\n",
    "      # cambio_gesto = False\n",
    "\n",
    "      # print(ind_receptor)\n",
    "      folds_list[ind_receptor].append(secuencias_repeticiones_encontradas[i])\n",
    "      ind_receptor += 1\n",
    "\n",
    "  # lo que sigue tengo que pasarlo a funciones\n",
    "  fold_list0 = folds_list[0]\n",
    "  fold_list1 = folds_list[1]\n",
    "  fold_list2 = folds_list[2]\n",
    "  fold_list3 = folds_list[3]\n",
    "  fold_list4 = folds_list[4]\n",
    "  fold_list5 = folds_list[5]\n",
    "\n",
    "  flattened_fold_list0 = np.concatenate(fold_list0)\n",
    "  flattened_fold_list1 = np.concatenate(fold_list1)\n",
    "  flattened_fold_list2 = np.concatenate(fold_list2)\n",
    "  flattened_fold_list3 = np.concatenate(fold_list3)\n",
    "  flattened_fold_list4 = np.concatenate(fold_list4)\n",
    "  flattened_fold_list5 = np.concatenate(fold_list5)\n",
    "\n",
    "# Hasta acá es igual al código de prueba de funcionamiento\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# TODO pasar a un for lo que sigue\n",
    "\n",
    "# fold 0\n",
    "  secuencias_data_array_fold_0 = np.array(selected_secuencias_data_list)[flattened_fold_list0]\n",
    "  secuencias_labels_array_fold_0 = selected_secuencias_labels_array[flattened_fold_list0]\n",
    "\n",
    "  secuencias_data_list_fold_0 = list(secuencias_data_array_fold_0)\n",
    "  secuencias_labels_list_fold_0 = list(secuencias_labels_array_fold_0)\n",
    "\n",
    "  secuencias_weight_array_fold_0 = sample_weight[flattened_fold_list0]\n",
    "  secuencias_weight_list_fold_0 = list(secuencias_weight_array_fold_0)\n",
    "  \n",
    "# fold 1\n",
    "  secuencias_data_array_fold_1 = np.array(selected_secuencias_data_list)[flattened_fold_list1]\n",
    "  secuencias_labels_array_fold_1 = selected_secuencias_labels_array[flattened_fold_list1]\n",
    "\n",
    "  secuencias_data_list_fold_1 = list(secuencias_data_array_fold_1)\n",
    "  secuencias_labels_list_fold_1 = list(secuencias_labels_array_fold_1)\n",
    "\n",
    "  secuencias_weight_array_fold_1 = sample_weight[flattened_fold_list1]\n",
    "  secuencias_weight_list_fold_1 = list(secuencias_weight_array_fold_1)\n",
    "\n",
    "# fold 2\n",
    "  secuencias_data_array_fold_2 = np.array(selected_secuencias_data_list)[flattened_fold_list2]\n",
    "  secuencias_labels_array_fold_2 = selected_secuencias_labels_array[flattened_fold_list2]\n",
    "  secuencias_data_list_fold_2 = list(secuencias_data_array_fold_2)\n",
    "  secuencias_labels_list_fold_2 = list(secuencias_labels_array_fold_2)\n",
    "\n",
    "  secuencias_weight_array_fold_2 = sample_weight[flattened_fold_list2]\n",
    "  secuencias_weight_list_fold_2 = list(secuencias_weight_array_fold_2)\n",
    "\n",
    "# fold 3\n",
    "  secuencias_data_array_fold_3 = np.array(selected_secuencias_data_list)[flattened_fold_list3]\n",
    "  secuencias_labels_array_fold_3 = selected_secuencias_labels_array[flattened_fold_list3]\n",
    "  secuencias_data_list_fold_3 = list(secuencias_data_array_fold_3)\n",
    "  secuencias_labels_list_fold_3 = list(secuencias_labels_array_fold_3)\n",
    "\n",
    "  secuencias_weight_array_fold_3 = sample_weight[flattened_fold_list3]\n",
    "  secuencias_weight_list_fold_3 = list(secuencias_weight_array_fold_3)\n",
    "\n",
    "# fold 4\n",
    "  secuencias_data_array_fold_4 = np.array(selected_secuencias_data_list)[flattened_fold_list4]\n",
    "  secuencias_labels_array_fold_4 = selected_secuencias_labels_array[flattened_fold_list4]\n",
    "  secuencias_data_list_fold_4 = list(secuencias_data_array_fold_4)\n",
    "  secuencias_labels_list_fold_4 = list(secuencias_labels_array_fold_4)\n",
    "\n",
    "  secuencias_weight_array_fold_4 = sample_weight[flattened_fold_list4]\n",
    "  secuencias_weight_list_fold_4 = list(secuencias_weight_array_fold_4)\n",
    "\n",
    "# fold 5\n",
    "  secuencias_data_array_fold_5 = np.array(selected_secuencias_data_list)[flattened_fold_list5]\n",
    "  secuencias_labels_array_fold_5 = selected_secuencias_labels_array[flattened_fold_list5]\n",
    "  secuencias_data_list_fold_5 = list(secuencias_data_array_fold_5)\n",
    "  secuencias_labels_list_fold_5 = list(secuencias_labels_array_fold_5)\n",
    "\n",
    "  secuencias_weight_array_fold_5 = sample_weight[flattened_fold_list5]\n",
    "  secuencias_weight_list_fold_5 = list(secuencias_weight_array_fold_5)\n",
    "\n",
    "  sample_weight_tensor_0 = tf.convert_to_tensor(secuencias_weight_array_fold_0, dtype=tf.float32)\n",
    "  sample_weight_dataset_0 = tf.data.Dataset.from_tensor_slices(sample_weight_tensor_0)\n",
    "\n",
    "  # Convertir las listas en tensores de TensorFlow\n",
    "  tensores = tf.stack(secuencias_data_list_fold_0)\n",
    "  etiquetas = tf.convert_to_tensor(list(secuencias_labels_array_fold_0))\n",
    "\n",
    "  # Crear un dataset a partir de los tensores y etiquetas\n",
    "  input_dataset = tf.data.Dataset.from_tensor_slices((tensores, etiquetas))\n",
    "  rnn_dataset_j = tf.data.Dataset.zip((input_dataset, sample_weight_dataset_0))\n",
    "  rnn_dataset_j_def = rnn_dataset_j.map(combine_features_labels_weights)\n",
    "\n",
    "  if guardar_ds:\n",
    "    rnn_dataset_j_def.save(nombre_exp+str(j)+\"_weighted_fold_rep_0\")\n",
    "\n",
    "\n",
    "  # secuencias_labels_array_fold_1 = secuencias_labels_array_fold_1 - 1\n",
    "  sample_weight_tensor_1 = tf.convert_to_tensor(secuencias_weight_array_fold_1, dtype=tf.float32)\n",
    "  sample_weight_dataset_1 = tf.data.Dataset.from_tensor_slices(sample_weight_tensor_1)\n",
    "\n",
    "  # Convertir las listas en tensores de TensorFlow\n",
    "  tensores = tf.stack(secuencias_data_list_fold_1)\n",
    "  etiquetas = tf.convert_to_tensor(list(secuencias_labels_array_fold_1))\n",
    "  # Crear un dataset a partir de los tensores y etiquetas\n",
    "  input_dataset = tf.data.Dataset.from_tensor_slices((tensores, etiquetas))\n",
    "  rnn_dataset_j = tf.data.Dataset.zip((input_dataset, sample_weight_dataset_1))\n",
    "  rnn_dataset_j_def = rnn_dataset_j.map(combine_features_labels_weights)\n",
    "\n",
    "  if guardar_ds:\n",
    "    rnn_dataset_j_def.save(nombre_exp+str(j)+\"_weighted_fold_rep_1\")\n",
    "\n",
    "\n",
    "  sample_weight_tensor_2 = tf.convert_to_tensor(secuencias_weight_array_fold_2, dtype=tf.float32)\n",
    "  sample_weight_dataset_2 = tf.data.Dataset.from_tensor_slices(sample_weight_tensor_2)\n",
    "  # Convertir las listas en tensores de TensorFlow\n",
    "  tensores = tf.stack(secuencias_data_list_fold_2)\n",
    "  etiquetas = tf.convert_to_tensor(list(secuencias_labels_array_fold_2))\n",
    "\n",
    "  # Crear un dataset a partir de los tensores y etiquetas\n",
    "  input_dataset = tf.data.Dataset.from_tensor_slices((tensores, etiquetas))\n",
    "  rnn_dataset_j = tf.data.Dataset.zip((input_dataset, sample_weight_dataset_2))\n",
    "  rnn_dataset_j_def = rnn_dataset_j.map(combine_features_labels_weights)\n",
    "\n",
    "  if guardar_ds:\n",
    "    rnn_dataset_j_def.save(nombre_exp+str(j)+\"_weighted_fold_rep_2\")\n",
    "\n",
    "\n",
    "  # secuencias_labels_array_fold_3 = secuencias_labels_array_fold_3 - 1\n",
    "  sample_weight_tensor_3 = tf.convert_to_tensor(secuencias_weight_array_fold_3, dtype=tf.float32)\n",
    "  sample_weight_dataset_3 = tf.data.Dataset.from_tensor_slices(sample_weight_tensor_3)\n",
    "\n",
    "  # Convertir las listas en tensores de TensorFlow\n",
    "  tensores = tf.stack(secuencias_data_list_fold_3)\n",
    "  etiquetas = tf.convert_to_tensor(list(secuencias_labels_array_fold_3))\n",
    "\n",
    "  # Crear un dataset a partir de los tensores y etiquetas\n",
    "  input_dataset = tf.data.Dataset.from_tensor_slices((tensores, etiquetas))\n",
    "  rnn_dataset_j = tf.data.Dataset.zip((input_dataset, sample_weight_dataset_3))\n",
    "  rnn_dataset_j_def = rnn_dataset_j.map(combine_features_labels_weights)\n",
    "\n",
    "  if guardar_ds:\n",
    "    rnn_dataset_j_def.save(nombre_exp+str(j)+\"_weighted_fold_rep_3\")\n",
    "\n",
    "  # secuencias_labels_array_fold_4 = secuencias_labels_array_fold_4 - 1\n",
    "  sample_weight_tensor_4 = tf.convert_to_tensor(secuencias_weight_array_fold_4, dtype=tf.float32)\n",
    "  sample_weight_dataset_4 = tf.data.Dataset.from_tensor_slices(sample_weight_tensor_4)\n",
    "  # Convertir las listas en tensores de TensorFlow\n",
    "  tensores = tf.stack(secuencias_data_list_fold_4)\n",
    "  etiquetas = tf.convert_to_tensor(list(secuencias_labels_array_fold_4))\n",
    "\n",
    "  # Crear un dataset a partir de los tensores y etiquetas\n",
    "  input_dataset = tf.data.Dataset.from_tensor_slices((tensores, etiquetas))\n",
    "  # # Guardar el dataset\n",
    "  # input_dataset.save(nombre_exp+str(j)+\"_fold_rep_4\")\n",
    "  rnn_dataset_j = tf.data.Dataset.zip((input_dataset, sample_weight_dataset_4))\n",
    "  rnn_dataset_j_def = rnn_dataset_j.map(combine_features_labels_weights)\n",
    "\n",
    "  if guardar_ds:\n",
    "    rnn_dataset_j_def.save(nombre_exp+str(j)+\"_weighted_fold_rep_4\")\n",
    "\n",
    "  # secuencias_labels_array_fold_5 = secuencias_labels_array_fold_5 - 1\n",
    "  sample_weight_tensor_5 = tf.convert_to_tensor(secuencias_weight_array_fold_5, dtype=tf.float32)\n",
    "  sample_weight_dataset_5 = tf.data.Dataset.from_tensor_slices(sample_weight_tensor_5)\n",
    "  # Convertir las listas en tensores de TensorFlow\n",
    "  tensores = tf.stack(secuencias_data_list_fold_5)\n",
    "  etiquetas = tf.convert_to_tensor(list(secuencias_labels_array_fold_5))\n",
    "  # Crear un dataset a partir de los tensores y etiquetas\n",
    "  input_dataset = tf.data.Dataset.from_tensor_slices((tensores, etiquetas))\n",
    "\n",
    "  rnn_dataset_j = tf.data.Dataset.zip((input_dataset, sample_weight_dataset_5))\n",
    "  rnn_dataset_j_def = rnn_dataset_j.map(combine_features_labels_weights)\n",
    "\n",
    "  if guardar_ds:\n",
    "    rnn_dataset_j_def.save(nombre_exp+str(j)+\"_weighted_fold_rep_5\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "entorno_PFG_Molina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
